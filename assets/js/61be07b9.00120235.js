"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[9595],{5557(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"specs/migration","title":"09 - Migration & Data Portability","description":"Overview","source":"@site/docs/specs/09-migration.md","sourceDirName":"specs","slug":"/specs/migration","permalink":"/aeterna/docs/specs/migration","draft":false,"unlisted":false,"editUrl":"https://github.com/kikokikok/aeterna/tree/main/website/docs/specs/09-migration.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{},"sidebar":"docs","previous":{"title":"Adapter Architecture Specification","permalink":"/aeterna/docs/specs/adapter-architecture"}}');var i=t(4848),s=t(8453);const a={},o="09 - Migration & Data Portability",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Export Format Specification",id:"export-format-specification",level:2},{value:"Archive Structure",id:"archive-structure",level:3},{value:"Manifest Schema",id:"manifest-schema",level:3},{value:"Memory Export Format (JSONL)",id:"memory-export-format-jsonl",level:3},{value:"Knowledge Export Format (JSONL)",id:"knowledge-export-format-jsonl",level:3},{value:"Embeddings Format",id:"embeddings-format",level:3},{value:"Import Procedures",id:"import-procedures",level:2},{value:"Import Flow",id:"import-flow",level:3},{value:"Import API",id:"import-api",level:3},{value:"Import CLI",id:"import-cli",level:3},{value:"Provider Migration",id:"provider-migration",level:2},{value:"Mem0 to Letta Migration",id:"mem0-to-letta-migration",level:3},{value:"Step 1: Export from Mem0",id:"step-1-export-from-mem0",level:4},{value:"Step 2: Import to Letta",id:"step-2-import-to-letta",level:4},{value:"Letta to Mem0 Migration",id:"letta-to-mem0-migration",level:3},{value:"OpenMemory Migration",id:"openmemory-migration",level:3},{value:"Legacy System Migration",id:"legacy-system-migration",level:2},{value:"Custom Database Migration",id:"custom-database-migration",level:3},{value:"Elasticsearch Migration",id:"elasticsearch-migration",level:3},{value:"MongoDB Migration",id:"mongodb-migration",level:3},{value:"Data Validation",id:"data-validation",level:2},{value:"Validation Rules",id:"validation-rules",level:3},{value:"Integrity Verification",id:"integrity-verification",level:3},{value:"Rollback Procedures",id:"rollback-procedures",level:2},{value:"Pre-Migration Backup",id:"pre-migration-backup",level:3},{value:"Rollback Script",id:"rollback-script",level:3},{value:"Point-in-Time Recovery",id:"point-in-time-recovery",level:3},{value:"Migration Checklist",id:"migration-checklist",level:2},{value:"Pre-Migration",id:"pre-migration",level:3},{value:"During Migration",id:"during-migration",level:3},{value:"Post-Migration",id:"post-migration",level:3},{value:"Rollback Criteria",id:"rollback-criteria",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Debug Mode",id:"debug-mode",level:3},{value:"Related Specifications",id:"related-specifications",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",input:"input",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"09---migration--data-portability",children:"09 - Migration & Data Portability"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"status: draft\nversion: 0.1.0\ndate: 2026-01-07\ndepends_on:\n  - 02-memory-system.md\n  - 03-knowledge-repository.md\n  - 05-adapter-architecture.md\n"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"This specification defines data portability standards, migration procedures, and import/export formats for the Memory-Knowledge System. The goal is to ensure organizations can:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Switch providers"})," without data loss"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Export"})," data for backup or compliance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Import"})," from legacy systems"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Migrate"})," between deployment models"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"export-format-specification",children:"Export Format Specification"}),"\n",(0,i.jsx)(n.h3,{id:"archive-structure",children:"Archive Structure"}),"\n",(0,i.jsx)(n.p,{children:"All exports use a standardized archive format:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"memory-knowledge-export-{timestamp}/\n\u251c\u2500\u2500 manifest.json           # Export metadata\n\u251c\u2500\u2500 memory/\n\u2502   \u251c\u2500\u2500 memories.jsonl      # Memory records (JSON Lines)\n\u2502   \u251c\u2500\u2500 embeddings.bin      # Binary embeddings (optional)\n\u2502   \u2514\u2500\u2500 embeddings.meta.json # Embedding metadata\n\u251c\u2500\u2500 knowledge/\n\u2502   \u251c\u2500\u2500 items.jsonl         # Knowledge items\n\u2502   \u251c\u2500\u2500 history.jsonl       # Version history\n\u2502   \u2514\u2500\u2500 constraints.jsonl   # Constraint definitions\n\u251c\u2500\u2500 relationships/\n\u2502   \u251c\u2500\u2500 memory-knowledge.jsonl  # Pointer mappings\n\u2502   \u2514\u2500\u2500 hierarchy.jsonl     # Layer relationships\n\u2514\u2500\u2500 checksums.sha256        # Integrity verification\n"})}),"\n",(0,i.jsx)(n.h3,{id:"manifest-schema",children:"Manifest Schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "$schema": "https://memory-knowledge-spec.org/schemas/export-manifest-v1.json",\n  "version": "1.0.0",\n  "exportedAt": "2026-01-07T02:00:00Z",\n  "exportedBy": "system",\n  "source": {\n    "provider": "mem0",\n    "version": "0.1.0",\n    "organizationId": "org_xxx"\n  },\n  "scope": {\n    "layers": ["project", "team", "org"],\n    "dateRange": {\n      "from": "2025-01-01T00:00:00Z",\n      "to": "2026-01-07T00:00:00Z"\n    },\n    "filters": {\n      "agentIds": ["agent_1", "agent_2"],\n      "userIds": ["user_*"],\n      "types": ["adr", "policy", "pattern"]\n    }\n  },\n  "statistics": {\n    "memories": {\n      "total": 15420,\n      "byLayer": {\n        "agent": 8500,\n        "user": 4200,\n        "session": 2720\n      }\n    },\n    "knowledge": {\n      "total": 342,\n      "byType": {\n        "adr": 45,\n        "policy": 128,\n        "pattern": 97,\n        "spec": 72\n      }\n    },\n    "relationships": {\n      "pointers": 892,\n      "promotions": 156\n    }\n  },\n  "integrity": {\n    "algorithm": "sha256",\n    "checksumFile": "checksums.sha256"\n  }\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"memory-export-format-jsonl",children:"Memory Export Format (JSONL)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-jsonl",children:'{"id":"mem_abc123","agentId":"agent_1","userId":"user_1","sessionId":"ses_001","layer":"session","content":"User prefers TypeScript over JavaScript for new projects","metadata":{"category":"preferences","confidence":0.92},"embedding":{"model":"text-embedding-3-large","dimensions":3072,"vectorId":"vec_xyz"},"createdAt":"2026-01-05T10:30:00Z","updatedAt":"2026-01-05T10:30:00Z","sourceKnowledge":{"id":"adr-001-typescript-standard","contentHash":"abc123...","syncedAt":"2026-01-05T11:00:00Z"}}\n{"id":"mem_abc124","agentId":"agent_1","userId":"user_2","sessionId":"ses_002","layer":"user","content":"Security team requires all API endpoints to use JWT authentication","metadata":{"category":"security","confidence":0.98},"embedding":{"model":"text-embedding-3-large","dimensions":3072,"vectorId":"vec_xyz2"},"createdAt":"2026-01-06T14:20:00Z","updatedAt":"2026-01-06T14:20:00Z","sourceKnowledge":null}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"knowledge-export-format-jsonl",children:"Knowledge Export Format (JSONL)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-jsonl",children:'{"id":"adr-001-typescript-standard","type":"adr","layer":"org","title":"Use TypeScript for All New Projects","summary":"Standardize on TypeScript for improved type safety and developer experience","content":"# ADR-001: TypeScript Standard\\n\\n## Status\\nAccepted\\n\\n## Context\\n...","contentHash":"sha256:abc123def456...","severity":"info","status":"accepted","tags":["typescript","standards","tooling"],"constraints":[{"operator":"must_use","target":"file","pattern":"*.ts","appliesTo":["src/**"],"severity":"warn","message":"Use TypeScript for source files"}],"metadata":{"author":"tech-lead","reviewers":["architect","security"]},"version":3,"createdAt":"2025-06-15T09:00:00Z","updatedAt":"2026-01-02T16:30:00Z","promotedAt":"2025-07-01T10:00:00Z","promotedFrom":"project"}\n{"id":"policy-auth-jwt","type":"policy","layer":"company","title":"JWT Authentication Required","summary":"All API endpoints must use JWT for authentication","content":"# Policy: JWT Authentication\\n\\n## Scope\\n...","contentHash":"sha256:def789ghi012...","severity":"block","status":"active","tags":["security","authentication","api"],"constraints":[{"operator":"must_match","target":"code","pattern":"@Authenticated|@JwtAuth|requireAuth","appliesTo":["**/controllers/**","**/routes/**"],"severity":"block","message":"API endpoints must have authentication"}],"metadata":{"complianceRef":"SOC2-AC-1"},"version":1,"createdAt":"2025-03-10T11:00:00Z","updatedAt":"2025-03-10T11:00:00Z","promotedAt":null,"promotedFrom":null}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"embeddings-format",children:"Embeddings Format"}),"\n",(0,i.jsx)(n.p,{children:"Binary format for efficient storage:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"// embeddings.meta.json\ninterface EmbeddingsMeta {\n  model: string;\n  dimensions: number;\n  count: number;\n  format: 'float32' | 'float16';\n  byteOrder: 'little-endian' | 'big-endian';\n  index: Array<{\n    id: string;\n    offset: number;  // Byte offset in .bin file\n  }>;\n}\n\n// embeddings.bin: Raw float32 vectors concatenated\n// Each vector: dimensions * 4 bytes (float32)\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"import-procedures",children:"Import Procedures"}),"\n",(0,i.jsx)(n.h3,{id:"import-flow",children:"Import Flow"}),"\n",(0,i.jsx)(n.mermaid,{value:"sequenceDiagram\n    participant User\n    participant Importer\n    participant Validator\n    participant Memory as Memory Service\n    participant Knowledge as Knowledge Service\n    participant Vector as Vector DB\n    \n    User->>Importer: Upload export archive\n    Importer->>Importer: Extract archive\n    Importer->>Validator: Validate manifest\n    Validator->>Validator: Check schema version\n    Validator->>Validator: Verify checksums\n    Validator--\x3e>Importer: Validation result\n    \n    alt Validation failed\n        Importer--\x3e>User: Error: validation failed\n    else Validation passed\n        Importer->>Knowledge: Import knowledge items\n        Knowledge->>Knowledge: Check for conflicts\n        Knowledge--\x3e>Importer: Knowledge import result\n        \n        Importer->>Memory: Import memories\n        Memory->>Vector: Import embeddings\n        Memory--\x3e>Importer: Memory import result\n        \n        Importer->>Importer: Restore relationships\n        Importer--\x3e>User: Import complete\n    end"}),"\n",(0,i.jsx)(n.h3,{id:"import-api",children:"Import API"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"interface ImportOptions {\n  // Archive source\n  source: string | ReadableStream;  // File path or stream\n  \n  // Conflict resolution\n  conflictStrategy: 'skip' | 'overwrite' | 'rename' | 'merge';\n  \n  // Scope filtering\n  filter?: {\n    layers?: string[];\n    types?: string[];\n    dateRange?: { from: Date; to: Date };\n  };\n  \n  // Mapping transformations\n  transforms?: {\n    agentIdMap?: Record<string, string>;\n    userIdMap?: Record<string, string>;\n    layerMap?: Record<string, string>;\n  };\n  \n  // Options\n  dryRun?: boolean;\n  validateOnly?: boolean;\n  embeddings?: 'import' | 'regenerate' | 'skip';\n}\n\ninterface ImportResult {\n  success: boolean;\n  statistics: {\n    memories: {\n      imported: number;\n      skipped: number;\n      failed: number;\n      conflicts: number;\n    };\n    knowledge: {\n      imported: number;\n      skipped: number;\n      failed: number;\n      conflicts: number;\n    };\n    relationships: {\n      restored: number;\n      broken: number;  // References to missing items\n    };\n  };\n  errors: ImportError[];\n  warnings: ImportWarning[];\n  duration: number;\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"import-cli",children:"Import CLI"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Basic import\nmk-import ./export-2026-01-07.tar.gz\n\n# Dry run to see what would happen\nmk-import ./export.tar.gz --dry-run\n\n# Import with conflict handling\nmk-import ./export.tar.gz --conflict-strategy=merge\n\n# Import only specific layers\nmk-import ./export.tar.gz --filter-layers=project,team\n\n# Import with ID remapping\nmk-import ./export.tar.gz \\\n  --agent-id-map=\'{"old_agent":"new_agent"}\' \\\n  --user-id-map=\'{"old_user":"new_user"}\'\n\n# Regenerate embeddings during import\nmk-import ./export.tar.gz --embeddings=regenerate\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"provider-migration",children:"Provider Migration"}),"\n",(0,i.jsx)(n.h3,{id:"mem0-to-letta-migration",children:"Mem0 to Letta Migration"}),"\n",(0,i.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Source: Mem0"\n        M0[Mem0 Cloud]\n        M0DB[(Mem0 Storage)]\n    end\n    \n    subgraph "Migration"\n        EX[Export]\n        TR[Transform]\n        IM[Import]\n    end\n    \n    subgraph "Target: Letta"\n        LT[Letta Server]\n        LTDB[(Local Storage)]\n    end\n    \n    M0 --\x3e EX\n    M0DB --\x3e EX\n    EX --\x3e TR\n    TR --\x3e IM\n    IM --\x3e LT\n    IM --\x3e LTDB'}),"\n",(0,i.jsx)(n.h4,{id:"step-1-export-from-mem0",children:"Step 1: Export from Mem0"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# export_mem0.py\nfrom mem0 import MemoryClient\nimport json\n\nclient = MemoryClient(api_key="your-api-key")\n\n# Export all memories\nmemories = client.get_all(\n    output_format="v1.1"  # Standard export format\n)\n\n# Write to JSONL\nwith open(\'memory/memories.jsonl\', \'w\') as f:\n    for memory in memories:\n        f.write(json.dumps(transform_mem0_to_standard(memory)) + \'\\n\')\n\ndef transform_mem0_to_standard(mem0_memory):\n    """Transform Mem0 format to standard export format."""\n    return {\n        "id": mem0_memory["id"],\n        "agentId": mem0_memory.get("agent_id", "default"),\n        "userId": mem0_memory.get("user_id"),\n        "sessionId": mem0_memory.get("metadata", {}).get("session_id"),\n        "layer": infer_layer(mem0_memory),\n        "content": mem0_memory["memory"],  # Mem0 uses \'memory\' field\n        "metadata": mem0_memory.get("metadata", {}),\n        "embedding": {\n            "model": "text-embedding-3-small",\n            "dimensions": 1536,\n            "vectorId": mem0_memory.get("id")\n        },\n        "createdAt": mem0_memory["created_at"],\n        "updatedAt": mem0_memory.get("updated_at", mem0_memory["created_at"]),\n        "sourceKnowledge": None\n    }\n'})}),"\n",(0,i.jsx)(n.h4,{id:"step-2-import-to-letta",children:"Step 2: Import to Letta"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# import_letta.py\nfrom letta import create_client\nimport json\n\nclient = create_client()\n\n# Create agent if needed\nagent = client.create_agent(\n    name="migrated-agent",\n    memory_blocks=[\n        {"label": "human", "value": ""},\n        {"label": "persona", "value": ""}\n    ]\n)\n\n# Import memories\nwith open(\'memory/memories.jsonl\', \'r\') as f:\n    for line in f:\n        memory = json.loads(line)\n        \n        # Letta uses archival memory for long-term storage\n        client.insert_archival_memory(\n            agent_id=agent.id,\n            memory=transform_standard_to_letta(memory)\n        )\n\ndef transform_standard_to_letta(standard_memory):\n    """Transform standard format to Letta format."""\n    return {\n        "text": standard_memory["content"],\n        "metadata": {\n            **standard_memory["metadata"],\n            "original_id": standard_memory["id"],\n            "layer": standard_memory["layer"],\n            "migrated_at": datetime.utcnow().isoformat()\n        }\n    }\n'})}),"\n",(0,i.jsx)(n.h3,{id:"letta-to-mem0-migration",children:"Letta to Mem0 Migration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# export_letta.py\nfrom letta import create_client\nimport json\n\nclient = create_client()\n\n# Get all agents\nagents = client.list_agents()\n\nall_memories = []\nfor agent in agents:\n    # Export archival memory\n    archival = client.get_archival_memory(\n        agent_id=agent.id,\n        limit=10000\n    )\n    \n    for memory in archival:\n        all_memories.append(transform_letta_to_standard(memory, agent))\n\n# Write export\nwith open('memory/memories.jsonl', 'w') as f:\n    for memory in all_memories:\n        f.write(json.dumps(memory) + '\\n')\n"})}),"\n",(0,i.jsx)(n.h3,{id:"openmemory-migration",children:"OpenMemory Migration"}),"\n",(0,i.jsx)(n.p,{children:"For self-hosted OpenMemory instances:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Export using OpenMemory CLI\nopenmemory export \\\n  --format=standard \\\n  --output=./export \\\n  --include-embeddings\n\n# Import to new instance\nopenmemory import \\\n  --source=./export \\\n  --target-url=https://new-instance.example.com \\\n  --conflict-strategy=merge\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"legacy-system-migration",children:"Legacy System Migration"}),"\n",(0,i.jsx)(n.h3,{id:"custom-database-migration",children:"Custom Database Migration"}),"\n",(0,i.jsx)(n.p,{children:"For organizations with memories in custom databases:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"// migration-adapter.ts\ninterface LegacyMemory {\n  // Define your legacy schema\n  record_id: number;\n  user_identifier: string;\n  memory_text: string;\n  category: string;\n  created_date: Date;\n}\n\ninterface MigrationAdapter {\n  connect(): Promise<void>;\n  fetchBatch(offset: number, limit: number): Promise<LegacyMemory[]>;\n  getTotal(): Promise<number>;\n  close(): Promise<void>;\n}\n\n// Example: PostgreSQL adapter\nclass PostgresLegacyAdapter implements MigrationAdapter {\n  private pool: Pool;\n  \n  async connect() {\n    this.pool = new Pool({\n      connectionString: process.env.LEGACY_DB_URL\n    });\n  }\n  \n  async fetchBatch(offset: number, limit: number): Promise<LegacyMemory[]> {\n    const result = await this.pool.query(\n      `SELECT * FROM memories ORDER BY record_id LIMIT $1 OFFSET $2`,\n      [limit, offset]\n    );\n    return result.rows;\n  }\n  \n  async getTotal(): Promise<number> {\n    const result = await this.pool.query('SELECT COUNT(*) FROM memories');\n    return parseInt(result.rows[0].count);\n  }\n  \n  async close() {\n    await this.pool.end();\n  }\n}\n\n// Migration runner\nasync function migrateLegacy(\n  adapter: MigrationAdapter,\n  transformer: (legacy: LegacyMemory) => StandardMemory,\n  targetClient: MemoryKnowledgeClient\n) {\n  await adapter.connect();\n  \n  const total = await adapter.getTotal();\n  const batchSize = 1000;\n  let processed = 0;\n  \n  while (processed < total) {\n    const batch = await adapter.fetchBatch(processed, batchSize);\n    \n    const standardMemories = batch.map(transformer);\n    \n    await targetClient.memory.bulkAdd(standardMemories);\n    \n    processed += batch.length;\n    console.log(`Progress: ${processed}/${total} (${(processed/total*100).toFixed(1)}%)`);\n  }\n  \n  await adapter.close();\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"elasticsearch-migration",children:"Elasticsearch Migration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"// elasticsearch-adapter.ts\nimport { Client } from '@elastic/elasticsearch';\n\nclass ElasticsearchLegacyAdapter implements MigrationAdapter {\n  private client: Client;\n  private scrollId: string | null = null;\n  \n  async connect() {\n    this.client = new Client({\n      node: process.env.ELASTICSEARCH_URL\n    });\n  }\n  \n  async fetchBatch(offset: number, limit: number) {\n    if (offset === 0) {\n      // Initial query\n      const response = await this.client.search({\n        index: 'memories',\n        scroll: '5m',\n        size: limit,\n        body: {\n          query: { match_all: {} },\n          sort: [{ created_at: 'asc' }]\n        }\n      });\n      this.scrollId = response._scroll_id;\n      return response.hits.hits.map(h => h._source);\n    } else {\n      // Scroll query\n      const response = await this.client.scroll({\n        scroll_id: this.scrollId!,\n        scroll: '5m'\n      });\n      this.scrollId = response._scroll_id;\n      return response.hits.hits.map(h => h._source);\n    }\n  }\n  \n  async getTotal() {\n    const response = await this.client.count({ index: 'memories' });\n    return response.count;\n  }\n  \n  async close() {\n    if (this.scrollId) {\n      await this.client.clearScroll({ scroll_id: this.scrollId });\n    }\n    await this.client.close();\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"mongodb-migration",children:"MongoDB Migration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"// mongodb-adapter.ts\nimport { MongoClient, Db } from 'mongodb';\n\nclass MongoLegacyAdapter implements MigrationAdapter {\n  private client: MongoClient;\n  private db: Db;\n  \n  async connect() {\n    this.client = new MongoClient(process.env.MONGODB_URL!);\n    await this.client.connect();\n    this.db = this.client.db('legacy');\n  }\n  \n  async fetchBatch(offset: number, limit: number) {\n    return this.db.collection('memories')\n      .find({})\n      .sort({ _id: 1 })\n      .skip(offset)\n      .limit(limit)\n      .toArray();\n  }\n  \n  async getTotal() {\n    return this.db.collection('memories').countDocuments();\n  }\n  \n  async close() {\n    await this.client.close();\n  }\n}\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"data-validation",children:"Data Validation"}),"\n",(0,i.jsx)(n.h3,{id:"validation-rules",children:"Validation Rules"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"interface ValidationRule {\n  field: string;\n  check: (value: any) => boolean;\n  severity: 'error' | 'warning';\n  message: string;\n}\n\nconst validationRules: ValidationRule[] = [\n  // Required fields\n  {\n    field: 'id',\n    check: (v) => typeof v === 'string' && v.length > 0,\n    severity: 'error',\n    message: 'ID is required and must be non-empty string'\n  },\n  {\n    field: 'content',\n    check: (v) => typeof v === 'string' && v.length > 0,\n    severity: 'error',\n    message: 'Content is required'\n  },\n  {\n    field: 'layer',\n    check: (v) => ['agent', 'user', 'session', 'project', 'team', 'org', 'company'].includes(v),\n    severity: 'error',\n    message: 'Layer must be valid hierarchy level'\n  },\n  \n  // Format validation\n  {\n    field: 'createdAt',\n    check: (v) => !isNaN(Date.parse(v)),\n    severity: 'error',\n    message: 'createdAt must be valid ISO 8601 date'\n  },\n  \n  // Warnings\n  {\n    field: 'embedding',\n    check: (v) => v !== null && v.dimensions > 0,\n    severity: 'warning',\n    message: 'Missing embedding will require regeneration'\n  },\n  {\n    field: 'metadata',\n    check: (v) => typeof v === 'object',\n    severity: 'warning',\n    message: 'Metadata should be an object'\n  }\n];\n\nasync function validateExport(exportPath: string): Promise<ValidationResult> {\n  const errors: ValidationError[] = [];\n  const warnings: ValidationWarning[] = [];\n  \n  // Validate manifest\n  const manifest = await readJson(`${exportPath}/manifest.json`);\n  if (!isValidManifest(manifest)) {\n    errors.push({ type: 'manifest', message: 'Invalid manifest schema' });\n  }\n  \n  // Validate checksums\n  const checksumValid = await verifyChecksums(`${exportPath}/checksums.sha256`);\n  if (!checksumValid) {\n    errors.push({ type: 'integrity', message: 'Checksum verification failed' });\n  }\n  \n  // Validate memories\n  const memoriesFile = `${exportPath}/memory/memories.jsonl`;\n  let lineNum = 0;\n  for await (const line of readLines(memoriesFile)) {\n    lineNum++;\n    const memory = JSON.parse(line);\n    \n    for (const rule of validationRules) {\n      if (!rule.check(memory[rule.field])) {\n        const item = { line: lineNum, field: rule.field, message: rule.message };\n        if (rule.severity === 'error') {\n          errors.push(item);\n        } else {\n          warnings.push(item);\n        }\n      }\n    }\n  }\n  \n  return {\n    valid: errors.length === 0,\n    errors,\n    warnings\n  };\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"integrity-verification",children:"Integrity Verification"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# checksums.sha256 format\nsha256sum memory/memories.jsonl\nsha256sum memory/embeddings.bin\nsha256sum knowledge/items.jsonl\nsha256sum knowledge/history.jsonl\n\n# Verification script\n#!/bin/bash\nset -e\n\ncd "$EXPORT_DIR"\n\necho "Verifying checksums..."\nsha256sum -c checksums.sha256\n\necho "Validating JSON..."\nfor f in memory/*.jsonl knowledge/*.jsonl; do\n  echo "Checking $f..."\n  while IFS= read -r line; do\n    echo "$line" | jq -e . > /dev/null || {\n      echo "Invalid JSON in $f"\n      exit 1\n    }\n  done < "$f"\ndone\n\necho "All validations passed"\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"rollback-procedures",children:"Rollback Procedures"}),"\n",(0,i.jsx)(n.h3,{id:"pre-migration-backup",children:"Pre-Migration Backup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# pre-migration-backup.sh\n\nBACKUP_DIR="/backups/pre-migration-$(date +%Y%m%d-%H%M%S)"\nmkdir -p "$BACKUP_DIR"\n\necho "Creating pre-migration backup..."\n\n# Backup PostgreSQL\npg_dump -h "$DB_HOST" -U "$DB_USER" memory > "$BACKUP_DIR/memory.sql"\npg_dump -h "$DB_HOST" -U "$DB_USER" knowledge > "$BACKUP_DIR/knowledge.sql"\n\n# Backup Qdrant\ncurl -X POST "http://$QDRANT_HOST:6333/collections/memories/snapshots" \\\n  -o "$BACKUP_DIR/qdrant-memories.snapshot"\ncurl -X POST "http://$QDRANT_HOST:6333/collections/knowledge/snapshots" \\\n  -o "$BACKUP_DIR/qdrant-knowledge.snapshot"\n\n# Create manifest\ncat > "$BACKUP_DIR/manifest.json" <<EOF\n{\n  "created": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",\n  "reason": "pre-migration",\n  "components": ["postgresql", "qdrant"]\n}\nEOF\n\necho "Backup complete: $BACKUP_DIR"\n'})}),"\n",(0,i.jsx)(n.h3,{id:"rollback-script",children:"Rollback Script"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# rollback.sh\n\nBACKUP_DIR=$1\n\nif [ -z "$BACKUP_DIR" ]; then\n  echo "Usage: rollback.sh <backup-dir>"\n  exit 1\nfi\n\necho "WARNING: This will restore from $BACKUP_DIR"\necho "All current data will be LOST."\nread -p "Continue? (yes/no) " confirm\n\nif [ "$confirm" != "yes" ]; then\n  echo "Rollback cancelled"\n  exit 0\nfi\n\necho "Stopping services..."\ndocker-compose stop memory-service knowledge-service sync-service\n\necho "Restoring PostgreSQL..."\npsql -h "$DB_HOST" -U "$DB_USER" -c "DROP DATABASE IF EXISTS memory"\npsql -h "$DB_HOST" -U "$DB_USER" -c "CREATE DATABASE memory"\npsql -h "$DB_HOST" -U "$DB_USER" memory < "$BACKUP_DIR/memory.sql"\n\npsql -h "$DB_HOST" -U "$DB_USER" -c "DROP DATABASE IF EXISTS knowledge"\npsql -h "$DB_HOST" -U "$DB_USER" -c "CREATE DATABASE knowledge"\npsql -h "$DB_HOST" -U "$DB_USER" knowledge < "$BACKUP_DIR/knowledge.sql"\n\necho "Restoring Qdrant..."\ncurl -X DELETE "http://$QDRANT_HOST:6333/collections/memories"\ncurl -X DELETE "http://$QDRANT_HOST:6333/collections/knowledge"\n\ncurl -X PUT "http://$QDRANT_HOST:6333/collections/memories/snapshots/recover" \\\n  -H "Content-Type: application/json" \\\n  -d "{\\"location\\": \\"$BACKUP_DIR/qdrant-memories.snapshot\\"}"\ncurl -X PUT "http://$QDRANT_HOST:6333/collections/knowledge/snapshots/recover" \\\n  -H "Content-Type: application/json" \\\n  -d "{\\"location\\": \\"$BACKUP_DIR/qdrant-knowledge.snapshot\\"}"\n\necho "Starting services..."\ndocker-compose up -d memory-service knowledge-service sync-service\n\necho "Rollback complete"\n'})}),"\n",(0,i.jsx)(n.h3,{id:"point-in-time-recovery",children:"Point-in-Time Recovery"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"interface PITROptions {\n  targetTime: Date;\n  targetConsistency: 'strict' | 'relaxed';\n}\n\nasync function pointInTimeRecovery(options: PITROptions): Promise<RecoveryResult> {\n  const { targetTime, targetConsistency } = options;\n  \n  // Find closest backup before target time\n  const backups = await listBackups();\n  const baseBackup = backups\n    .filter(b => b.timestamp < targetTime)\n    .sort((a, b) => b.timestamp.getTime() - a.timestamp.getTime())[0];\n  \n  if (!baseBackup) {\n    throw new Error('No backup found before target time');\n  }\n  \n  // Restore base backup\n  await restoreBackup(baseBackup.path);\n  \n  // Replay WAL to target time (PostgreSQL)\n  if (targetConsistency === 'strict') {\n    await replayWAL(baseBackup.timestamp, targetTime);\n  }\n  \n  // Rebuild vector indexes\n  await rebuildVectorIndexes();\n  \n  return {\n    restoredFrom: baseBackup.timestamp,\n    restoredTo: targetTime,\n    consistency: targetConsistency\n  };\n}\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"migration-checklist",children:"Migration Checklist"}),"\n",(0,i.jsx)(n.h3,{id:"pre-migration",children:"Pre-Migration"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Create full backup of source system"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Verify backup integrity"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Test restore procedure in staging"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Document current data statistics"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Plan maintenance window"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Notify stakeholders"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"during-migration",children:"During Migration"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Stop source system writes (if possible)"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Run export"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Validate export integrity"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Run import with ",(0,i.jsx)(n.code,{children:"--dry-run"})]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Review dry-run results"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Run actual import"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Verify data counts match"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"post-migration",children:"Post-Migration"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Run validation queries"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Test sample queries"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Verify relationship integrity"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Run application smoke tests"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Monitor error rates"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," Keep backup for 30 days"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"rollback-criteria",children:"Rollback Criteria"}),"\n",(0,i.jsx)(n.p,{children:"Trigger rollback if:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Data count mismatch > 1%"}),"\n",(0,i.jsx)(n.li,{children:"Query latency increased > 50%"}),"\n",(0,i.jsx)(n.li,{children:"Error rate increased > 5%"}),"\n",(0,i.jsx)(n.li,{children:"Critical functionality broken"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Issue"}),(0,i.jsx)(n.th,{children:"Cause"}),(0,i.jsx)(n.th,{children:"Solution"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Checksum mismatch"}),(0,i.jsx)(n.td,{children:"Corrupted download"}),(0,i.jsx)(n.td,{children:"Re-download archive"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"ID conflicts"}),(0,i.jsx)(n.td,{children:"Duplicate IDs"}),(0,i.jsxs)(n.td,{children:["Use ",(0,i.jsx)(n.code,{children:"--conflict-strategy=rename"})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Missing embeddings"}),(0,i.jsx)(n.td,{children:"Different model"}),(0,i.jsxs)(n.td,{children:["Use ",(0,i.jsx)(n.code,{children:"--embeddings=regenerate"})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"UTF-8 errors"}),(0,i.jsx)(n.td,{children:"Encoding mismatch"}),(0,i.jsx)(n.td,{children:"Convert source to UTF-8"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Memory limit"}),(0,i.jsx)(n.td,{children:"Large batch"}),(0,i.jsx)(n.td,{children:"Reduce batch size"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Slow import"}),(0,i.jsx)(n.td,{children:"Missing indexes"}),(0,i.jsx)(n.td,{children:"Create indexes before import"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"debug-mode",children:"Debug Mode"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Enable verbose logging\nexport MK_LOG_LEVEL=debug\n\n# Import with detailed logging\nmk-import ./export.tar.gz --verbose --log-file=migration.log\n\n# Check specific record\nmk-import --validate-record=\'{"id":"mem_abc123",...}\'\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"related-specifications",children:"Related Specifications"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/aeterna/docs/specs/memory-system",children:"02-memory-system.md"})," - Memory schema"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/aeterna/docs/specs/knowledge-repository",children:"03-knowledge-repository.md"})," - Knowledge schema"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/aeterna/docs/specs/adapter-architecture",children:"05-adapter-architecture.md"})," - Provider interfaces"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/aeterna/docs/specs/deployment",children:"08-deployment.md"})," - Deployment patterns"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>a,x:()=>o});var r=t(6540);const i={},s=r.createContext(i);function a(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);